---
title: "Store Sales Forecasting"
subtitle: "Kaggle Competition"
author: "Mateo"
format: 
  html:
    toc: true
    embed-resources: true
---


# Initial setup

## Packages
```{r}
#install.packages("devtools")
#install.packages("tictoc")
#install.packages("future")
#install.packages("parallelly")
#install.packages("anomalize")

```

## Libraries
```{r}
#| label: pkgs
#| message: false
#| warning: false

library(conflicted)
## install kaggler package from github
# pak::pak("mkearney/kaggler")
library(future)
library(furrr)
library(tidymodels)
library(tidyverse)
library(tsibble)
library(feasts)
#library(anomalize)
library(fable)
library(fabletools)
library(purrr)
library(dplyr)

conflicts_prefer(
  lubridate::date(),
  dplyr::filter()
)
```

### I use multisession:
```{r}
options(future.globals.maxSize = 2 * 1024^3)  # 2 GB
plan(multisession, workers = 4) # Usa varios n√∫cleos
```

# Data import

## `train.csv`

### Convert to a `tsibble`

```{r}
#| label: train
train_tsbl <- read_csv("data/train.csv", 
                       show_col_types = FALSE) |> 
  as_tsibble(index = date, key = c(store_nbr, family)) |> 
  select(-onpromotion)

train_tsbl
```

### Checking for gaps

```{r}
train_tsbl |> 
  has_gaps()
```
All of them have implicit gaps in time. Below we'll find where are such gaps:

```{r}
gaps <- train_tsbl |> 
  count_gaps(.full = TRUE) |> 
  distinct(.from)

gaps
```

It's Christmas. We'll fill the implicit gaps and set to zero its value.

```{r}
#| label: fill_gaps
train_tsbl <- train_tsbl |> 
  fill_gaps(.full = TRUE, 
            sales = 0L)

train_tsbl
```
## additional data

### `holidays_events.csv`

```{r}
holidays <- read_csv("data/holidays_events.csv", show_col_types = FALSE)
holidays
```

### `stores.csv`

```{r}
stores <- read_csv("data/stores.csv", show_col_types = FALSE)
stores
```

### `oil.csv`

```{r}
oil <- read_csv("data/oil.csv", show_col_types = FALSE) |> 
  as_tsibble(index = date) |> 
  fill_gaps(.full = TRUE) |> 
  fill(dcoilwtico, .direction = "downup")
oil 
```

### Joining the data in one `tsibble`

```{r}
train_tsbl <- train_tsbl |> 
  left_join(oil, by = "date")
train_tsbl
```

### `test.csv`

```{r}
test <- read_csv("data/test.csv", show_col_types = FALSE) |> 
  select(-onpromotion)
test
```


# Exploratory analysis

## Products with no sales

We'll check if there are any family products not being sold in specific stores:

```{r}
ceros <- train_tsbl |> 
  as_tibble() |> 
  group_by(store_nbr, family) |> 
  summarise(sales = sum(sales), .groups = "drop") |> 
  filter(sales == 0) |> 
  select(-sales)

ceros
```
There are `{r} nrow(ceros)` series with no sales in the training set. We'll use a **NAIVE** model to forecast them.The time series that do have sales will be stored in `series_tsbl`:

```{r}
series_tsbl <- train_tsbl |> 
  anti_join(ceros, by = c("store_nbr", "family"))

series_tsbl
```

and the ones without sales in `series_ceros`:

```{r}
series_ceros <- train_tsbl |> 
  anti_join(series_tsbl, by = c("date", "store_nbr", "family"))
series_ceros
```

The forecasts for such series are done below:

```{r}
series_ceros_fit <- series_ceros |> 
  model(naive = NAIVE(sales))
series_ceros_fit

series_ceros_fcst <- series_ceros_fit |> 
  forecast(h = 16)

series_ceros_fcst
```

First, several questions arise about our data, two in particular:
1. Will there be time series with many zeros at the beginning?
2. Will there be time series with only zeros at the end?

## We filter out the zeros

### First question:

```{r}
# We look for the first non-null value in each series:
primer_valor_no_cero <- series_tsbl |> 
  group_by(store_nbr, family) |>  
  filter(sales != 0) |>  
  slice_min(date) |>  # We choose the first row (the first non-zero value)
  select(store_nbr, family, date) |> 
  arrange(desc(date)) |> 
  rename(fecha_inicio = date)
primer_valor_no_cero
```

```{r}
# Filter the series by removing data prior to the first non-zero value
ceros_series_filtradas <- series_tsbl |> 
  left_join(primer_valor_no_cero, by = c("store_nbr", "family")) |>  
  filter(date < fecha_inicio) |>  # We perform the join only when the date is less than 'fecha_inicio'
  select(-fecha_inicio)  # We remove the 'fecha_inicio' column from the resulting table

# Filtramos y guardamos las series importantes
series_tsbl_filtered <- series_tsbl |> 
  anti_join(ceros_series_filtradas, by = c("date", "store_nbr", "family"))

series_tsbl_filtered
```

Now for the second question, simply filter out those time series where the last 30 days had sales = 0.
This makes your Forecast equal to 0.

### Second question:
```{r}
# Last month:
series_tsbl_ultimo_mes <- series_tsbl_filtered |>
  filter(date >= as.Date("2017-07-15"), date <= as.Date("2017-08-15"))

# Let's see which ones are all zero. 
# I'll also take this opportunity to exclude from my forecasts those that had very few sales.
series_pocas_ventas_ultimo_mes <- series_tsbl_ultimo_mes |>
  as_tibble() |> 
  group_by(store_nbr, family) |> 
  summarise(sales = sum(sales), .groups = "drop") |> 
  filter(sales < 4) |> 
  select(-sales)
series_pocas_ventas_ultimo_mes
```

So let's make their forecast:
```{r}
# First we will remove the data from these 82 time series from the important tsibble:
series_tsbl_importantes <- series_tsbl_filtered |> 
  anti_join(series_pocas_ventas_ultimo_mes, by = c("store_nbr", "family"))

# Now we work with 'series_pocas_ventas_ultimo_mes'
series_pobres_ultimo_mes <- series_pocas_ventas_ultimo_mes |>
  left_join(series_tsbl_ultimo_mes, by = c("store_nbr", "family")) |>
  as_tsibble(index = date, key = c(store_nbr, family))

# Now the forecast:
series_pobres_ultimo_mes_fit <- series_pobres_ultimo_mes |> 
  model(naive = NAIVE(sales))

series_pobres_ultimo_mes_fcst <- series_pobres_ultimo_mes_fit |> 
  forecast(h = 16)

series_pobres_ultimo_mes_fcst
```

## Simple models (Poor series)
Now we have the condition that to use certain models we need at least 2 complete cycles to get a good grip on seasonality, so we will filter out those series that do not have it and use simple models to forecast them

```{r}
casos_no_bianuales <- primer_valor_no_cero |>
  filter(fecha_inicio > as.Date("2015-07-11")) |>
  arrange(desc(fecha_inicio))

series_no_bianuales_tslb <- series_tsbl_importantes |>
  inner_join(casos_no_bianuales, by = c("store_nbr","family")) |>
  as_tsibble(index = date, key = c(store_nbr, family))
series_no_bianuales_tslb

series_tsbl_importantes2 <- series_tsbl_importantes |>
  anti_join(series_no_bianuales_tslb,by = c("store_nbr","family"))
series_tsbl_importantes2
```

Now that I've filtered some time series based on some conditions related to business rules.
I can filter out those time series that predict well using a simple model.
```{r}
# First I do the Train series
series_no_bianuales_tslb_train <- series_no_bianuales_tslb |>
  filter_index(. ~ "2017-07-30")

# Now I do the Test series
series_no_bianuales_tslb_test <- series_no_bianuales_tslb |>
  filter_index("2017-07-31" ~ .)

# I propose the models to be used by series:  
tictoc::tic()
series_no_bianuales_tslb_train_fit <- series_no_bianuales_tslb_train |> 
  model(
    ets = ETS(sales),
    snaive = SNAIVE(sales),
  )
tictoc::toc()

# Forecast:
tictoc::tic()
series_no_bianuales_tslb_train_fcst <- series_no_bianuales_tslb_train_fit |> 
  forecast(h = 16) 
tictoc::toc()
series_no_bianuales_tslb_train_fcst

# Combine forecast and test to compare:
comparacion_modelos_basicos <- series_no_bianuales_tslb_train_fcst |>
  left_join(series_no_bianuales_tslb_test, by = c("store_nbr", "family", "date"), suffix = c("_pred", "_real")) |>
  select(-sales_pred) |>
  rename(sales_pred = .mean) |>
  rename(modelo = .model)
comparacion_modelos_basicos 

# I calculate the RMSLE
tictoc::tic()
rmsle_por_serie_modelos_basicos <- comparacion_modelos_basicos |>
  as_tibble() |>
  group_by(store_nbr, family, modelo) |> 
  summarise(
    rmsle = sqrt( mean( 
      (log1p(sales_pred) - log1p(sales_real))^2 # formula
    , na.rm = TRUE ) ),
    .groups = "drop"
  ) |>
  arrange(rmsle) # Order by rmsle
tictoc::toc()

rmsle_por_serie_modelos_basicos
```

### Now I select the best model and I do the forecast:
```{r}
mejores_modelos_basicos <- rmsle_por_serie_modelos_basicos |>
  group_by(store_nbr, family) |> 
  slice_min(rmsle, n = 1) |>
  ungroup() |> 
  arrange(rmsle)

# Ahora que identificamos los modelos importantes, procederemos a usarlos en la serie original:
tictoc::tic()
series_no_bianuales_tslb_fit <- series_no_bianuales_tslb |> 
  model(
    ets = ETS(sales),
    snaive = SNAIVE(sales),
  )
tictoc::toc()

# luego los forecast:

tictoc::tic()
series_no_bianuales_tslb_fcst_varios <- series_no_bianuales_tslb_fit |> 
  forecast(h = 16)
tictoc::toc()
series_no_bianuales_tslb_fcst_varios

# Selecciono los mejores:
series_no_bianuales_fcst <- series_no_bianuales_tslb_fcst_varios |> 
  left_join(mejores_modelos_basicos, by = c("store_nbr", "family")) |>
  filter(.model == modelo) |>
  select(store_nbr, family, date, sales, .mean, rmsle, modelo)
series_no_bianuales_fcst
```

## Cero Analysis
The reason for this analysis is that you will probably use ARIMA at some point, therefore it is necessary not to have so many zeros in the series
```{r}
series_ultimo_anio <- series_tsbl_importantes2 |>
  filter_index("2016-08-15" ~ .)

series_ordenadas_por_ceros <- series_ultimo_anio |>
  as_tibble() |>
  group_by(store_nbr, family) |>
  summarise(
    cantidad_ceros = sum(sales == 0, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(desc(cantidad_ceros))  # Order by 'cantidad_ceros'

series_ordenadas_por_ceros
```

### I filter the series that have more than 15 zeros in the last year and use simple models
```{r}
series_con_muchos_ceros <- series_ordenadas_por_ceros |>
  filter(cantidad_ceros > 15)

# Select data from tsibble
series_con_muchos_ceros_tsbl <- series_tsbl_importantes2 |>
  inner_join(series_con_muchos_ceros, by = c("store_nbr", "family")) |>
  as_tsibble(index = date, key = c(store_nbr, family))
series_con_muchos_ceros_tsbl

# create 'series_tsbl_importantes3'
series_tsbl_importantes3 <- series_tsbl_importantes2 |>
  anti_join(series_con_muchos_ceros_tsbl, by = c("store_nbr", "family"))
series_tsbl_importantes3

```
Now that I filtered the series by those that did have many zeros, so as not to analyze them later with ARIMA, what I will do is propose a list of simple models:
  1. ETS
  2. SNAIVE
```{r}
# First I do the Train series
series_con_muchos_ceros_tsbl_train <- series_con_muchos_ceros_tsbl |>
  filter_index(. ~ "2017-07-30")

# Now I do the Test series
series_con_muchos_ceros_tsbl_test <- series_con_muchos_ceros_tsbl |>
  filter_index("2017-07-31" ~ .)

# I propose the models to be used by series:  
tictoc::tic()
series_con_muchos_ceros_tsbl_train_fit <- series_con_muchos_ceros_tsbl_train |> 
  model(
    ets = ETS(sales),
    LOGets = ETS(log(sales)),
    #snaive = SNAIVE(sales)
  )
tictoc::toc()

# Forecast:
tictoc::tic()
series_con_muchos_ceros_tsbl_train_fcst <- series_con_muchos_ceros_tsbl_train_fit |> 
  forecast(h = 16) 
tictoc::toc()
series_con_muchos_ceros_tsbl_train_fcst

# Combine forecast and test to compare:
comparacion_modelos_con_muchos_ceros <- series_con_muchos_ceros_tsbl_train_fcst |>
  left_join(series_con_muchos_ceros_tsbl_test, by = c("store_nbr", "family", "date"), suffix = c("_pred", "_real")) |>
  select(-sales_pred) |>
  rename(sales_pred = .mean) |>
  rename(modelo = .model)
comparacion_modelos_con_muchos_ceros 

# I calculate the RMSLE
tictoc::tic()
rmsle_por_serie_modelos_con_muchos_ceros <- comparacion_modelos_con_muchos_ceros |>
  as_tibble() |>
  group_by(store_nbr, family, modelo) |> 
  summarise(
    rmsle = sqrt( mean( 
      (log1p(sales_pred) - log1p(sales_real))^2 # formula
    , na.rm = TRUE ) ),
    .groups = "drop"
  ) |>
  arrange(rmsle) # Order by rmsle
tictoc::toc()

rmsle_por_serie_modelos_con_muchos_ceros
```

### Now I select the best model and I do the forecast:
```{r}
mejores_modelos_con_muchos_ceros <- rmsle_por_serie_modelos_con_muchos_ceros |>
  group_by(store_nbr, family) |> 
  # Sort by rmsle first, then prioritize ets
  arrange(rmsle, desc(modelo == "ets")) |>
  slice_head(n = 1) |> 
  ungroup()

# Now that we've identified the important models, we'll proceed to use them in the original series:
tictoc::tic()
series_con_muchos_ceros_tsbl_fit <- series_con_muchos_ceros_tsbl |> 
  model(
    ets = ETS(sales),
    LOGets = ETS(log(sales)),
    #snaive = SNAIVE(sales)
  )
tictoc::toc()

# forecast:
tictoc::tic()
series_con_muchos_ceros_tsbl_fcst_varios <- series_con_muchos_ceros_tsbl_fit |> 
  forecast(h = 16)
tictoc::toc()
series_con_muchos_ceros_tsbl_fcst_varios

# Selecciono los mejores:
series_con_muchos_ceros_tsbl_fcst <- series_con_muchos_ceros_tsbl_fcst_varios |> 
  as_tibble() |>
  left_join(mejores_modelos_con_muchos_ceros, by = c("store_nbr", "family")) |>
  filter(.model == modelo) |>
  select(store_nbr, family, date, sales, .mean, rmsle, modelo)
series_con_muchos_ceros_tsbl_fcst
```

```{r}
series_con_muchos_ceros_tsbl |>
  features(sales, guerrero) |>
  arrange(desc(lambda_guerrero))
```




## Feature Analysis
```{r}
features <- series_tsbl_importantes3 |>
  features(sales, list(
    feat_stl,
    feat_acf,
    feat_pacf,
    guerrero
  )) |>
  arrange(desc(acf1))

features
```
















# Benchmark model

## Fit

```{r}
tictoc::tic()
series_tsbl_fit <- series_tsbl_importantes3 |> 
  model(
    ets = ETS(sales)
  )
tictoc::toc()

series_tsbl_fit
```

```{r}
tictoc::tic()
fabletools::accuracy(series_tsbl_fit)
tictoc::toc()
```


## Forecast

```{r}
tictoc::tic()
series_tsbl_fcst <- series_tsbl_fit |> 
  forecast(h = 16)
tictoc::toc()

series_tsbl_fcst
```

### Joining forecasts

```{r}
full_fcst <- series_tsbl_fcst |> 
  bind_rows(series_ceros_fcst) |>
  bind_rows(series_pobres_ultimo_mes_fcst) |>
  bind_rows(series_no_bianuales_fcst) |>
  bind_rows(series_con_muchos_ceros_tsbl_fcst)

full_fcst
```


## Submission

```{r}
submission <- full_fcst |> 
  left_join(test, by = c("date", "store_nbr", "family")) |> 
  as_tibble() |>
  select(id, .mean) |>
  rename(sales = .mean)
submission

submission |> 
  write_csv("VersionesPrueba/submission_M9.csv")
```

## we finished using parallelism

```{r}
plan(sequential)
```